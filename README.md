# RLProject

## Первое занятие
Выбранная среда - Atari Tetris.
Характеристики среды:
- env.action_space = Discrete(5)
- env.metadata = {'render_modes': ['human', 'rgb_array'], 'obs_types': {'rgb', 'ram', 'grayscale'}}
- env.observation_space = Box(0, 255, (210, 160, 3), uint8)
- env.reward_range = (-inf, inf)
- env.spec = EnvSpec(id='ALE/Tetris-v5', entry_point='shimmy.atari_env:AtariEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'game': 'tetris', 'obs_type': 'rgb', 'repeat_action_probability': 0.25, 'full_action_space': False, 'frameskip': 4, 'max_num_frames_per_episode': 108000}, namespace='ALE', name='Tetris', version=5, additional_wrappers=(), vector_entry_point=None)
Среда дает ревард только тогда, когда убирается линия.

В первом опыте мы прописали e-жудную стратегию и выстроили 30 траекторий. На их основе посчитали точеченую и интервальную оценку, получив следующие результаты:
- Точечная оценка: 4.272333333333287
- Интервальная оценка: (3.0843223594152898, 5.460344307251284)

Первым алгоритмом для реализации стал Q-learning из группы Value Based алгоритмов. Также на рассмотрении находился и сарса, но Q-learning обновляет политику, что позволяет качеству обучения быть выше, несмотря на замедление времени обучения. Выбрали его, поскольку относительно прост в реализации и позволяет агенту постепенно улучшать свою политику поведения на основе опыта. 
Q - обучение - это алгоритм, в котором Q-функция обновляется по формуле:

$$
Q(s,a)=Q(s,a)+\alpha(r+\gamma\max_{a'}Q(s',a')-Q(s,a))
$$

Здесь $s'$ — состояние, в которое окружающая среда переходит из состояния $s$ после действия $a$,

$r$ — полученное при этом вознаграждение,

$\alpha$ — скорость обучения,

$\gamma$ — коэффициент обесценивания.

Величина $\max\limits_{a'} Q(s',a')$ означает, что стратегия жадная, т.е. для генерации обучающих данных выбирается действие в состоянии $s'$ с наибольшим значением Q-функции. В алгоритме $Q$-обучения действия выбираются с помощью $\varepsilon$-жадной стратегии.

В $Q$-обучении с аппроксимацией функции ошибка:

$$
\delta = r+\gamma V(s_{t+1})-V(s_t)=r+\gamma\max_{a'}V(s')-V(s_t)
$$

Цель обучения — минимизировать член ошибки, сведя его к нулю, т.е. оценка $V(s_t)$ должна удовлетворять уравнению

$$
V(s_t)=r+\gamma\max_{a'}V(s')
$$

В нашем случае мы даем ревард, равный 100 когда сжигается линия, а 0.1 реварда за каждый кадр до терминального состояния. Такой ревард выбран для того, чтобы обучить алгоритм держаться в игре как можно дольше.

Проведем эксперименты с нашим алгоритмом. Количество эпизодов в первом эксперименте равно 500, а во втором 2000, lr=0.03, epsilon = 0.1, n_features = 100.
Результаты не сильно радуют и в первом эксперименте не удалось срезать ни одну линию.
Во втором лучше, удалось срезать одну линию и то в самом начале. В дальнейших эпизодах он вышел на линию реварда 4.5. Следующие показатели были получены для второго эксперимента:
- Точечная оценка: 4.814170000000177
- Интервальная оценка: (-0.27846430731094607, 9.9068043073113)